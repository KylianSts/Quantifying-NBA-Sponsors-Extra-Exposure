"""
YOLO OBB Model Training Pipeline with Integrated EDA

This script provides a complete pipeline for training YOLO Oriented Bounding Box (OBB)
models with comprehensive Exploratory Data Analysis (EDA). It analyzes the dataset,
generates visualization plots, and trains the model with optimal configurations.

Key features:
- Converts Label Studio JSON annotations to structured DataFrame
- Performs comprehensive EDA with multiple visualizations
- Analyzes class distribution, bounding box areas, rotations, and spatial locations
- Generates heatmaps and scatter plots for object localization analysis
- Trains YOLO OBB models with GPU acceleration (if available)
- Saves all analysis results and training outputs to organized directories
- Supports both rectangle-with-rotation and pre-computed points annotations
"""

import os
import json
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import torch
from ultralytics import YOLO
from typing import List, Dict

# ============================================================================
# CONFIGURATION
# ============================================================================

# Path to Label Studio JSON export containing all annotations
JSON_SOURCE_FILE = "Data/json_files/yolo_label_studio.json"

# Path to YOLO dataset configuration file (generated by dataset converter)
DATASET_YAML_PATH = 'Data/yolo_dataset/nba_sponsor_dataset.yaml'

# Base YOLO model to use for fine-tuning (can be 'n', 's', 'm', 'l', or 'x' variants)
MODEL_TO_USE = 'Models/yolo/yolov8n-obb.pt'  # 'n' = nano (fastest), 'x' = extra large (most accurate)

# Training hyperparameters
EPOCHS = 5  # Number of training epochs (increase for better results, e.g., 100-300)
IMAGE_SIZE = 640  # Input image size in pixels (must be multiple of 32)
BATCH_SIZE = 8  # Number of images per batch (reduce if GPU memory is limited)

# Output directory structure
PROJECT_NAME = 'Models/models_results/modelisation_v6'  # Parent folder for this dataset version
EXPERIMENT_NAME = 'yolov8n-obb_fine_tuned_v6'  # Specific name for THIS training run

# ============================================================================
# EDA FUNCTIONS
# ============================================================================

def get_bbox_from_points(points: list) -> dict:
    """
    Calculate bounding box properties from 4 corner points.
    
    Computes the axis-aligned bounding box dimensions and rotation angle
    from polygon corner coordinates. Used for annotations stored as points
    rather than x, y, width, height format.
    
    Args:
        points: List of 4 [x, y] coordinate pairs representing box corners
    
    Returns:
        Dictionary containing x, y, width, height, and rotation (in degrees)
    """
    # Convert to numpy array for vectorized operations
    points = np.array(points)
    
    # Calculate axis-aligned bounding box
    x_min, y_min = np.min(points, axis=0)
    x_max, y_max = np.max(points, axis=0)
    width = x_max - x_min
    height = y_max - y_min
    
    # Calculate rotation from first edge vector
    edge_vector = points[1] - points[0]  # Vector from first to second corner
    angle_rad = np.arctan2(edge_vector[1], edge_vector[0])  # Angle in radians
    angle_deg = np.rad2deg(angle_rad)  # Convert to degrees
    
    return {
        'x': x_min, 
        'y': y_min, 
        'width': width, 
        'height': height, 
        'rotation': angle_deg
    }


def extract_annotations(annotation_list: list, image_id: str) -> list:
    """
    Extract annotations from Label Studio format, handling mixed annotation types.
    
    Supports two formats:
    1. Rectangle with x, y, width, height, rotation
    2. Pre-computed polygon points (4 corners)
    
    Args:
        annotation_list: List of Label Studio annotation objects
        image_id: Identifier for the image these annotations belong to
    
    Returns:
        List of dictionaries, each containing one bounding box annotation
    """
    rows = []
    
    # Process each annotation object
    for annotation in annotation_list:
        # Extract individual bounding box results
        for result in annotation.get('result', []):
            value = result.get('value', {})
            
            # Skip if not a rectangle annotation
            if 'rectanglelabels' not in value:
                continue
            
            # Extract class label
            class_name = value['rectanglelabels'][0]
            
            # Format 1: Standard rectangle with rotation
            if 'x' in value and 'rotation' in value:
                props = {k: value[k] for k in ['x', 'y', 'width', 'height', 'rotation']}
            
            # Format 2: Pre-computed corner points
            elif 'points' in value:
                props = get_bbox_from_points(value['points'])
            
            # Skip malformed annotations
            else:
                continue

            # Create row with all annotation properties
            row = {
                'image_id': image_id,
                'bbox_id': result.get('id'),
                'label': class_name,
                **props
            }
            rows.append(row)
    
    # Add empty row if no annotations found (image with no objects)
    if not rows:
        rows.append({
            'image_id': image_id,
            'bbox_id': None,
            'label': None,
            'x': None,
            'y': None,
            'width': None,
            'height': None,
            'rotation': None
        })
    
    return rows


def create_annotations_dataframe(json_path: str) -> pd.DataFrame:
    """
    Load Label Studio JSON and convert it to a structured DataFrame.
    
    Reads the Label Studio export format and converts all annotations into
    a flat DataFrame structure for easy analysis and visualization.
    
    Args:
        json_path: Path to Label Studio JSON export file
    
    Returns:
        DataFrame with columns: image_id, bbox_id, label, x, y, width, height, rotation
    """
    # Load Label Studio JSON
    data = pd.read_json(json_path)
    
    # Filter to only labeled images (those with at least one annotation)
    data_labeled = data[data['total_annotations'] > 0].copy()
    
    # Extract image IDs from image paths (remove extension and path)
    data_labeled['image_id'] = data_labeled['data'].map(
        lambda x: os.path.splitext(os.path.basename(x.get('image', '')))[0]
    )
    
    # Extract all annotations into flat structure
    all_rows = []
    for _, row in data_labeled.iterrows():
        extracted = extract_annotations(row['annotations'], row['image_id'])
        all_rows.extend(extracted)
    
    return pd.DataFrame(all_rows)


def run_and_save_eda(df_annotations: pd.DataFrame, save_dir: str) -> None:
    """
    Perform Exploratory Data Analysis and save all plots and statistics.
    
    Generates comprehensive visualizations and statistics including:
    - Dataset overview (number of images, videos, bounding boxes)
    - Class distribution bar plots
    - Bounding box area distributions (violin plots)
    - Rotation angle distributions
    - Spatial localization heatmaps and scatter plots (global and per-class)
    
    Skips if the output directory already exists to avoid regenerating.
    
    Args:
        df_annotations: DataFrame containing all annotations
        save_dir: Parent directory where EDA results will be saved
    
    Returns:
        None
    """
    print("\n" + "="*60)
    print("RUNNING EXPLORATORY DATA ANALYSIS (EDA)")
    print("="*60)
    
    # Create EDA output directory
    eda_save_path = os.path.join(save_dir, "data_exploration")
    
    # Skip if EDA already generated
    if os.path.exists(eda_save_path):
        print(f"Directory '{eda_save_path}' already exists. Skipping EDA generation.")
        return

    os.makedirs(eda_save_path, exist_ok=True)
    
    # Initialize statistics text file content
    stats_text = ""

    # ========================================================================
    # Dataset Overview Statistics
    # ========================================================================
    stats_text += "="*30 + "\nDATASET OVERVIEW\n" + "="*30 + "\n"
    stats_text += f"Number of unique images: {df_annotations['image_id'].nunique()}\n"
    
    # Count unique video sources (extract video ID from frame filenames)
    stats_text += f"Number of different videos: {df_annotations['image_id'].map(lambda x: str(x)[:str(x).find('frame_')] if 'frame_' in str(x) else 'N/A').nunique()}\n"
    stats_text += f"Total number of bounding boxes: {df_annotations['bbox_id'].nunique()}\n\n"
    
    # ========================================================================
    # Class Distribution Analysis
    # ========================================================================
    dist_labels = df_annotations.groupby("label")["image_id"].count()
    stats_text += "="*30 + "\nCLASS DISTRIBUTION\n" + "="*30 + "\n"
    stats_text += f"{dist_labels.to_string()}\n\n"

    # Create bar plot of class distribution
    plt.figure(figsize=(12, 7))
    sns.countplot(
        y=df_annotations['label'],
        order=df_annotations['label'].value_counts().index,
        palette="viridis"
    )
    plt.title("Number of Bounding Boxes per Class")
    plt.xlabel("Count")
    plt.ylabel("Class")
    plt.tight_layout()
    plt.savefig(os.path.join(eda_save_path, "class_distribution.png"))
    plt.close()

    # ========================================================================
    # Bounding Box Area Distribution Analysis
    # ========================================================================
    # Calculate area for each bounding box (percentage squared)
    df_annotations['area'] = df_annotations['width'] * df_annotations['height']
    
    stats_text += "="*30 + "\nBOUNDING BOX AREA STATS (%^2)\n" + "="*30 + "\n"
    stats_text += f"{df_annotations.groupby('label')['area'].describe().to_string()}\n\n"

    # Create violin plot of area distribution by class
    plt.figure(figsize=(12, 7))
    sns.violinplot(data=df_annotations, x='label', y='area', palette="plasma")
    plt.xlabel("Label")
    plt.ylabel("Bounding Box Area (%²)")
    plt.title("Distribution of Bounding Box Areas by Class")
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.savefig(os.path.join(eda_save_path, "area_distribution.png"))
    plt.close()
    
    # ========================================================================
    # Rotation Distribution Analysis
    # ========================================================================
    stats_text += "="*30 + "\nBOUNDING BOX ROTATION STATS (degrees)\n" + "="*30 + "\n"
    stats_text += f"{df_annotations.groupby('label')['rotation'].describe().to_string()}\n\n"

    # Create violin plot of rotation distribution by class
    plt.figure(figsize=(12, 7))
    sns.violinplot(data=df_annotations, x='label', y='rotation', palette="magma")
    plt.xlabel("Label")
    plt.ylabel("Rotation (degrees)")
    plt.title("Distribution of Bounding Box Rotations by Class")
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.savefig(os.path.join(eda_save_path, "rotation_distribution.png"))
    plt.close()

    # ========================================================================
    # Bounding Box Localization Analysis
    # ========================================================================
    # Calculate center coordinates for spatial analysis
    df_annotations['center_x'] = df_annotations['x'] + df_annotations['width'] / 2
    df_annotations['center_y'] = df_annotations['y'] + df_annotations['height'] / 2

    # Global heatmap showing all bounding box locations
    plt.figure(figsize=(14, 8))
    plt.hexbin(
        df_annotations['center_x'],
        df_annotations['center_y'],
        gridsize=30,
        cmap='rocket',
        mincnt=1  # Minimum count to display
    )
    plt.colorbar(label='Number of bounding boxes')
    plt.xlabel('Position X (%)')
    plt.ylabel('Position Y (%)')
    plt.title('Heatmap of Bounding Box Center Locations (All Classes)')
    plt.gca().invert_yaxis()  # Invert Y-axis to match image coordinates
    plt.tight_layout()
    plt.savefig(os.path.join(eda_save_path, "localization_heatmap_all.png"))
    plt.close()

    # Global scatter plot with class color coding
    plt.figure(figsize=(14, 8))
    sns.scatterplot(
        data=df_annotations.dropna(subset=['label']),
        x='center_x',
        y='center_y',
        hue='label',
        s=50,
        alpha=0.7,
        palette="viridis"
    )
    plt.xlabel('Position X (%)')
    plt.ylabel('Position Y (%)')
    plt.title('Scatter Plot of Bounding Box Locations by Class')
    plt.gca().invert_yaxis()  # Invert Y-axis to match image coordinates
    plt.legend(title='Label', bbox_to_anchor=(1.02, 1), loc='upper left')
    plt.tight_layout()
    plt.savefig(os.path.join(eda_save_path, "localization_scatterplot.png"))
    plt.close()
    
    # Per-class heatmaps showing spatial distribution of each object type
    for label in sorted(df_annotations['label'].dropna().unique()):
        df_label = df_annotations[df_annotations['label'] == label]
        
        plt.figure(figsize=(14, 8))
        plt.hexbin(
            df_label['center_x'],
            df_label['center_y'],
            gridsize=20,
            cmap='rocket',
            mincnt=1
        )
        plt.colorbar(label='Number of bounding boxes')
        plt.title(f'Localization Heatmap - {label}')
        plt.xlabel('Position X (%)')
        plt.ylabel('Position Y (%)')
        plt.gca().invert_yaxis()  # Invert Y-axis to match image coordinates
        plt.tight_layout()
        plt.savefig(os.path.join(eda_save_path, f"localization_heatmap_{label}.png"))
        plt.close()

    # ========================================================================
    # Save Text Statistics
    # ========================================================================
    with open(os.path.join(eda_save_path, "dataset_stats.txt"), "w") as f:
        f.write(stats_text)
    
    print(f"✓ EDA plots and stats saved to: {eda_save_path}")


# ============================================================================
# TRAINING FUNCTION
# ============================================================================

def train_yolo_model(
    model_path: str,
    dataset_yaml: str,
    epochs: int,
    image_size: int,
    batch_size: int,
    project_name: str,
    experiment_name: str
) -> YOLO:
    """
    Train the YOLO OBB model with specified parameters.
    
    Loads a pre-trained YOLO model and fine-tunes it on the custom dataset.
    Automatically uses GPU if available, otherwise falls back to CPU.
    
    Args:
        model_path: Path to base YOLO model weights (.pt file)
        dataset_yaml: Path to dataset configuration YAML file
        epochs: Number of training epochs
        image_size: Input image size (must be multiple of 32)
        batch_size: Number of images per batch
        project_name: Parent directory for saving results
        experiment_name: Specific name for this training run
    
    Returns:
        Trained YOLO model object
    """
    print("\n" + "="*60)
    print("STARTING YOLO MODEL TRAINING")
    print("="*60)
    print(f"Model: {os.path.basename(model_path)}")
    print(f"Epochs: {epochs}")
    print(f"Image size: {image_size}x{image_size}")
    print(f"Batch size: {batch_size}")
    print(f"Device: {'GPU (CUDA)' if torch.cuda.is_available() else 'CPU'}")
    print("="*60 + "\n")
    
    # Load base YOLO model
    model = YOLO(model_path)
    
    # Train model with specified parameters
    model.train(
        data=dataset_yaml,  # Dataset configuration
        epochs=epochs,  # Number of training epochs
        imgsz=image_size,  # Input image size
        batch=batch_size,  # Batch size
        project=project_name,  # Output directory
        name=experiment_name,  # Experiment name
        device='0' if torch.cuda.is_available() else 'cpu'  # Use GPU if available
    )
    
    return model


# ============================================================================
# MAIN FUNCTION
# ============================================================================

def main() -> None:
    """
    Main training pipeline with integrated EDA.
    
    Complete workflow:
    1. Load and convert Label Studio annotations to DataFrame
    2. Perform comprehensive Exploratory Data Analysis (EDA)
    3. Generate visualization plots and statistics
    4. Train YOLO OBB model on the dataset
    5. Save all results to organized directories
    
    Returns:
        None
    """
    print("="*60)
    print("YOLO OBB TRAINING PIPELINE")
    print("="*60)
    
    # ========================================================================
    # STEP 1: Data Preparation and EDA
    # ========================================================================
    print("\n[STEP 1/2] Data Analysis and Visualization")
    print("-"*60)
    
    try:
        # Convert Label Studio JSON to structured DataFrame
        df_annotations = create_annotations_dataframe(JSON_SOURCE_FILE)
        print(f"✓ Loaded {len(df_annotations)} annotations from {df_annotations['image_id'].nunique()} images")
        
        # Run EDA and save all visualizations
        run_and_save_eda(df_annotations, PROJECT_NAME)
        
    except FileNotFoundError:
        print(f"\n[FATAL ERROR] Annotation file not found at: {JSON_SOURCE_FILE}")
        print("Please ensure the Label Studio export exists at the specified path.")
        return
    
    except Exception as e:
        print(f"\n[FATAL ERROR] Failed during data analysis: {e}")
        return

    # ========================================================================
    # STEP 2: Model Training
    # ========================================================================
    print("\n[STEP 2/2] Model Training")
    print("-"*60)
    
    try:
        # Warn if no GPU available (training will be very slow)
        if not torch.cuda.is_available():
            print("\nWARNING: No CUDA-enabled GPU found. Training will be very slow on CPU.")
            print("Consider using Google Colab or a cloud GPU for faster training.\n")
        
        # Train YOLO model
        trained_model = train_yolo_model(
            model_path=MODEL_TO_USE,
            dataset_yaml=DATASET_YAML_PATH,
            epochs=EPOCHS,
            image_size=IMAGE_SIZE,
            batch_size=BATCH_SIZE,
            project_name=PROJECT_NAME,
            experiment_name=EXPERIMENT_NAME
        )
        
        # Display completion summary
        print("\n" + "="*60)
        print("TRAINING PIPELINE COMPLETED SUCCESSFULLY!")
        print("="*60)
        print(f"EDA results: {os.path.join(PROJECT_NAME, 'data_exploration')}")
        print(f"Training results: {os.path.join(PROJECT_NAME, EXPERIMENT_NAME)}")
        print(f"Best model weights: {os.path.join(PROJECT_NAME, EXPERIMENT_NAME, 'weights', 'best.pt')}")
        print("="*60 + "\n")
        
    except Exception as e:
        print(f"\n[FATAL ERROR] Training failed: {e}")
        print("Please check your dataset configuration and try again.")


if __name__ == '__main__':
    main()